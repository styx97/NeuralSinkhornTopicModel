job_name: "optimal-transport"

## These will be populated by the code
# Note the pipe | before `slurm_header` is necessary to parse as separate lines
templates:
    command: "/workspace/.conda/envs/ntm_optimal_transport/bin/python /workspace/rupak/ot/NeuralSinkhornTopicModel/main.py --config {config_path} --output_dir {output_dir}/"
    slurm_header: |
        #!/bin/bash
        #SBATCH --array=0-{n_jobs}%30
        #SBATCH --job-name={job_name}
        #SBATCH --output={log_dir}/{job_name}-%A-%a.log
        #SBATCH --partition=gpu
        #SBATCH --constraint=gpu-small
        #SBATCH --gpus-per-node=1
        #SBATCH --cpus-per-task=4
    # scheme for naming folders
    run_name: "{input_dir}/k-{num_topics}/ot/lr_{learning_rate}-epochs_{num_epochs}/{run_seeds}"

## Hyperparams. 
## note: use a dictionary {value: name} to assign names to values when formatting the `run_name`
hyper:
    input_dir: {
        "/workspace/topic-preprocessing/data/bills/processed/labeled/vocab_5k": "bills-labeled/vocab_5k",
        "/workspace/topic-preprocessing/data/bills/processed/labeled/vocab_15k": "bills-labeled/vocab_15k",
        "/workspace/topic-preprocessing/data/wikitext/processed/labeled/vocab_5k": "wikitext-labeled/vocab_5k",
        "/workspace/topic-preprocessing/data/wikitext/processed/labeled/vocab_15k": "wikitext-labeled/vocab_15k",
    }
    num_topics: [25, 50, 100, 200]
    #alpha_prior: [0.1, 0.01, 0.001]
    learning_rate: [0.01, 0.001]
    input_rec_loss_weight: [ 0.05, 0.07, 0.1, 0.2]
    sh_alpha: [100, 33, 20, 14, 10]
    #topic_word_regularization: [0.0, 0.01, 0.1, 1.0] # reguarization totally breaks when using softmax
    num_epochs: [50, 100]
    #epochs_to_anneal_bn: [0, 1, 100, 200]
    #epochs_to_anneal_kl: [100, 200]
    run_seeds: [42, 11235, 5591]

# Optional: filter out param values based on the values of other params
# constraints: 
#     - [epochs_to_anneal_bn, <, num_epochs]
#     - [epochs_to_anneal_kl, <, num_epochs]

# Optional: specify directories or filenames of code which will be copied to the ouput dir for posterity
# Git hashes will also be saved, if available, for these directories
code_locations:
    - /workspace/rupak/ot/NeuralSinkhornTopicModel/
# defaults 
params:
    train_path: train.dtm.npz
    eval_path: test.dtm.npz
    vocab_path: vocab.json
    temp_output_dir: /scratch/
    embeddings_model: glove
    
    sh_epsilon: 0.001
    sh_iterations: 50 
    learning_rate: 0.001
    batch_size: 200
    num_topics: 100
    random_seed: 42
    num_epochs: 50
    input_rec_loss_weight: 0.07
    sh_alpha: 20
    
    num_topics: 100
    hidden_sizes: (100,100)
    dropout: 0.2 
    batch_size: 64

    gpu: true


